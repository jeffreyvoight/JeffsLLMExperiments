from os import environ
from dotenv import load_dotenv
import psycopg2
import os
import json

load_dotenv()
OLLAMA_HOST=os.environ.get("OLLAMA_HOST")
DB_String=os.environ.get("DATABASE_CONNECTION_STRING")

def connect_db():
    return psycopg2.connect(DB_String)

# Create the table
def create_table(tablename):
    with connect_db() as connection:
        with connection.cursor() as cursor:
            # Install the ai extension and its dependencies
            cursor.execute("""
                        CREATE EXTENSION IF NOT EXISTS ai CASCADE;
                        """)
            cursor.execute(f"""
                           CREATE TABLE IF NOT EXISTS {tablename} (
                                id          BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,
                                filename    TEXT,
                                meta_data   JSON,
                                author      TEXT,
                                title       TEXT NOT NULL,
                                text        TEXT NOT NULL
                                )
                            """)

# Create the embedding table
def create_embedding_table(tablename, embedding_model, embedding_ctx_size, chunk_size, chunk_overlap):
    model = embedding_model.replace('-', '_')
    with connect_db() as connection:
        with connection.cursor() as cursor:
            cursor.execute(f"""
                            SELECT ai.create_vectorizer(
                                '{tablename}'::regclass,
                                    destination => '{tablename}_{model}_{chunk_size}',
                                    embedding => ai.embedding_ollama('{embedding_model}', {embedding_ctx_size}),
                                    chunking => ai.chunking_recursive_character_text_splitter('text', {chunk_size}, {chunk_overlap}, separators => array[E'\n;', ' '])
                                )
                            """)

def drop_text_tables(tablename):
     with connect_db() as connection:
        with connection.cursor() as cursor:
            cursor.execute(f"DROP TABLE {tablename} CASCADE;")
                     


# This method has an intentional lazy flaw in that if a record is already in the database, we just leave it alone for now.
def add_text_record(_text, tablename):
    print(_text.metadata)
    with db.connect_db() as connection:
        with connection.cursor() as cursor:
            cursor.execute(f"SELECT filename from {tablename} where filename = %s", [_text.filename])
            if(len(cursor.fetchall()) == 0):
                meta_data = json.dumps(_text.metadata)
                print(f"Adding file: {meta_data}\n")
                cursor.execute(f"INSERT into {tablename} (filename, author, title, meta_data, text)" \
                "VALUES (%s, %s, %s, %s, %s)", (_text.filename, _text.author, _text.title, meta_data, _text.contents))