{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafd66d5",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Rename compose.yaml.default to compose.yaml and edit it if needed. As it stands, it will launch the vectorizer, the database, and ollama.\n",
    "\n",
    "Execute this to start the containers:\n",
    "```sh\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### By the way, if you wreck your install and need to start over:\n",
    "```sh\n",
    "docker compose down\n",
    "docker volume rm pgai_data\n",
    "docker rm pgai_db_1 pgai_vectorizer-worker_1 pgai_ollama_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e64a24",
   "metadata": {},
   "source": [
    "### Setup your PSQL instance\n",
    "\n",
    "```sh\n",
    "docker compose exec db psql\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Install pgai\n",
    "```sql\n",
    "CREATE EXTENSION IF NOT EXISTS ai CASCADE;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "While you're there, create the database table\n",
    "```sql\n",
    "CREATE TABLE books (\n",
    "    id          TEXT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n",
    "    filename    TEXT,\n",
    "    author      TEXT,\n",
    "    title       TEXT NOT NULL,\n",
    "    text        TEXT NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "And, before you quit, create the vectorizer\n",
    "```sql\n",
    "SELECT ai.create_vectorizer(\n",
    "     'texts'::regclass,\n",
    "     destination => 'texts_embeddings',\n",
    "     embedding => ai.embedding_ollama('nomic-embed-text', 768),\n",
    "     chunking => ai.chunking_recursive_character_text_splitter('text')\n",
    ");\n",
    "```\n",
    "\n",
    "*Keep in mind that for other texts, you can customize the chunking and embedding models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faddfe8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you've got a spare terminal window, you can tail the vectorizer logs\n",
    "```sh\n",
    "docker compose logs -f vectorizer-worker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35645d",
   "metadata": {},
   "source": [
    "---\n",
    "Let's get our imports out of the way. Run this.\n",
    "\n",
    "In your virtual environment, run the following to install the necessary libraries.\n",
    "```sh\n",
    "python -m pip install numpy ollama langchain_text_splitters python-dotenv pandas psycopg2-binary Jinja2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274aac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from ollama import Client\n",
    "import time\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc409da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OLLAMA_HOST=os.environ.get(\"OLLAMA_HOST\")\n",
    "DB_String=os.environ.get(\"DATABASE_CONNECTION_STRING\")\n",
    "\n",
    "def connect_db():\n",
    "    return psycopg2.connect(DB_String)\n",
    "\n",
    "def create_table(tablename):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                        CREATE EXTENSION IF NOT EXISTS ai CASCADE;\n",
    "                        \"\"\")\n",
    "\n",
    "            cursor.execute(f\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS {tablename} (\n",
    "                            id          BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n",
    "                            filename    TEXT,\n",
    "                            author      TEXT,\n",
    "                            title       TEXT NOT NULL,\n",
    "                            text        TEXT NOT NULL\n",
    "                            )\n",
    "                        \"\"\")\n",
    "\n",
    "            cursor.execute(f\"\"\"\n",
    "                        SELECT count(*) from {tablename}_embeddings;\n",
    "                            \"\"\")\n",
    "            cursor.execute(f\"\"\"\n",
    "                            select count(*) from {tablename}_embeddings_store;\n",
    "                           \"\"\")\n",
    "            if(len(cursor.fetchall()) == 0):\n",
    "                cursor.execute(f\"\"\"\n",
    "                            SELECT ai.create_vectorizer(\n",
    "                                '{tablename}'::regclass,\n",
    "                                    destination => '{tablename}_embeddings',\n",
    "                                    embedding => ai.embedding_ollama('nomic-embed-text', 768),\n",
    "                                    chunking => ai.chunking_recursive_character_text_splitter('text', 500, 10, separators => array[E'\\n;', ' '])\n",
    "                                )\n",
    "                            \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a20fb4",
   "metadata": {},
   "source": [
    "Now, load some data into the **texts** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d66d99",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "connection to server at \"192.168.137.117\", port 5432 failed: Connection timed out (0x0000274C/10060)\n\tIs the server running on that host and accepting TCP/IP connections?\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     26\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(cursor.fetchall()) == \u001b[32m0\u001b[39m):\n\u001b[32m     27\u001b[39m                 cursor.execute(\u001b[33m\"\u001b[39m\u001b[33mINSERT into texts (filename, author, title, text)\u001b[39m\u001b[33m\"\u001b[39m \\\n\u001b[32m     28\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mVALUES (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, (_text.filename, _text.author, _text.title, _text.contents))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m files = load_directory(content_directory, \u001b[33m\"\u001b[39m\u001b[33m*.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Modify this     vv    to limit how many texts to vectorize\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mcreate_table\u001b[39m\u001b[34m(tablename)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_table\u001b[39m(tablename):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mconnect_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m connection.cursor() \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[32m     15\u001b[39m             cursor.execute(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m                        CREATE EXTENSION IF NOT EXISTS ai CASCADE;\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m                        \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mconnect_db\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect_db\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpsycopg2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDB_String\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeff\\IdeaProjects\\JeffsLLMExperiments\\.venv\\Lib\\site-packages\\psycopg2\\__init__.py:122\u001b[39m, in \u001b[36mconnect\u001b[39m\u001b[34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     kwasync[\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m] = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33masync_\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    121\u001b[39m dsn = _ext.make_dsn(dsn, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m conn = \u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconnection_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwasync\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cursor_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    124\u001b[39m     conn.cursor_factory = cursor_factory\n",
      "\u001b[31mOperationalError\u001b[39m: connection to server at \"192.168.137.117\", port 5432 failed: Connection timed out (0x0000274C/10060)\n\tIs the server running on that host and accepting TCP/IP connections?\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "content_directory = \"texts\"\n",
    "\n",
    "class Text:\n",
    "    def __init__(self, filename):\n",
    "        self.title = re.match(r\".+\\\\(.+)\\.txt$\", filename).group(1)\n",
    "        self.author = re.match(r\".+\\\\(\\w+)\\\\.+.txt\", filename).group(1)\n",
    "        self.filename = f\"{self.author}-{self.title}\"\n",
    "\n",
    "        with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "            file_contents = f.read()\n",
    "            self.contents = re.sub(r'[^\\S\\r\\n]+', \" \", file_contents)\n",
    "        # print(f\"{self.title} by {self.author}\")\n",
    "        # print(self.contents[:30])\n",
    "\n",
    "def load_directory(dirname, filter):\n",
    "    return list(Path(dirname).rglob(filter))\n",
    "\n",
    "# This method has an intentional lazy flaw in that if a record is already in the database, we just leave it alone for now.\n",
    "def add_text_record(_text):\n",
    "    print(_text.filename)\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT filename from texts where filename = %s\", [_text.filename])\n",
    "            if(len(cursor.fetchall()) == 0):\n",
    "                cursor.execute(\"INSERT into texts (filename, author, title, text)\" \\\n",
    "                \"VALUES (%s, %s, %s, %s)\", (_text.filename, _text.author, _text.title, _text.contents))\n",
    "\n",
    "create_table(content_directory)\n",
    "files = load_directory(content_directory, \"*.txt\")\n",
    "\n",
    "# Modify this     vv    to limit how many texts to vectorize\n",
    "for file in files[:4]:\n",
    "    text = Text(str(file))\n",
    "    # We aren't creating an array of Text objects because it could consume an\n",
    "    # outrageous amount of memory if there are tons of texts.\n",
    "    add_text_record(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec735ef0",
   "metadata": {},
   "source": [
    "Let's talk a bit about *Vectorization*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8531802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "496\n",
      "496\n",
      "497\n",
      "495\n",
      "---\n",
      "499\n",
      "496\n",
      "496\n",
      "497\n",
      "495\n",
      "---\n",
      "499\n",
      "496\n",
      "496\n",
      "499\n",
      "494\n"
     ]
    }
   ],
   "source": [
    "def gather_appropriate_chunks_by_author_metadata(author, prompt, count):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                           SELECT \n",
    "                           chunk,\n",
    "                           embedding <=> ai.ollama_embed('nomic-embed-text', %s) as distance\n",
    "                           FROM texts_embeddings\n",
    "                           WHERE author = %s\n",
    "                           ORDER BY distance\n",
    "                           LIMIT %s;\n",
    "                           \"\"\", (prompt, author, count))\n",
    "            return cursor.fetchall()    \n",
    "\n",
    "def gather_appropriate_chunks_by_title_metadata(title, prompt, count):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                           SELECT \n",
    "                           chunk,\n",
    "                           embedding <=> ai.ollama_embed('nomic-embed-text', %s) as distance\n",
    "                           FROM texts_embeddings\n",
    "                           WHERE title = %s\n",
    "                           ORDER BY distance\n",
    "                           LIMIT %s;\n",
    "                           \"\"\", (prompt, title, count))\n",
    "            return cursor.fetchall() \n",
    "\n",
    "def gather_inappropriate_chunks(prompt, count):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                           SELECT \n",
    "                           chunk,\n",
    "                           embedding <=> ai.ollama_embed('nomic-embed-text', %s) as distance\n",
    "                           FROM texts_embeddings\n",
    "                           ORDER BY distance\n",
    "                           LIMIT %s;\n",
    "                           \"\"\", (prompt, count))\n",
    "            return cursor.fetchall() \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "909f4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    host='http://192.168.137.117:11434',\n",
    ")\n",
    "prompt_size=8000\n",
    "chunking_size=500\n",
    "chunk_count = prompt_size // chunking_size\n",
    "embed_model = \"nomic-embed-text\"\n",
    "generate_model = \"gemma3:12b\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful reading assistant who answers questions\n",
    "based on snippets of text provided in context. Answer only using the context provided,\n",
    "being as concise as possible. If the answer isn't in the context, simply say so.\n",
    "Context:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(prompt, most_similar_chunks, model):\n",
    "    # client.pull(generate_model) # You really should pre-pull your models\n",
    "    \n",
    "    # most_similar_chunks = gather_inappropriate_chunks(prompt)\n",
    "    # for item in most_similar_chunks:\n",
    "    #     print(item[0])\n",
    "    # print(\"\\n\\n\\n\")\n",
    "    system_prompt = SYSTEM_PROMPT + \"\\n\".join(item[0] for item in most_similar_chunks)\n",
    "    # print(f\"{system_prompt}\\n\\n\")\n",
    "    \n",
    "    response = client.chat(\n",
    "        model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        stream = True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def stream_response(stream):\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00613f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter is \"youth, joy,\" and \"a little bird that has broken out of the egg.\" On the island, he is the \"Great White Father,\" who is looked up to and obeyed. He leads a band of lost boys and has the help of Tiger Lily and her braves. He also thins out the boys who are growing up and enforces rules they must follow."
     ]
    }
   ],
   "source": [
    "prompt = \"who is peter and what was his role on the island?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"peterpan\", prompt, 14) # 14 seems to be the sweetspot. any more and you over-run the context buffer and bad things happen.\n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b93a242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('When last we saw him he was stealing across the\\nisland with one finger to his lips and his dagger at the ready. He had\\nseen the crocodile pass by without noticing anything peculiar about it,\\nbut by and by he remembered that it had not been ticking. At first he\\nthought this eerie, but soon concluded rightly that the clock had run\\ndown.\\n\\nWithout giving a thought to what might be the feelings of a\\nfellow-creature thus abruptly deprived of its closest companion, Peter\\nbegan to consider how he could', 0.31846014408478407)\n",
      "('did not compete. For one thing he despised all mothers except\\nWendy, and for another he was the only boy on the island who could\\nneither write nor spell; not the smallest word. He was above all that\\nsort of thing.\\n\\nBy the way, the questions were all written in the past tense. What was\\nthe colour of Mother’s eyes, and so on. Wendy, you see, had been\\nforgetting, too.\\n\\nAdventures, of course, as we shall see, were of daily occurrence; but\\nabout this time Peter invented, with Wendy’s help, a new', 0.33544244831688075)\n",
      "---\n",
      "\n",
      "Peter was the only boy on the island who could neither write nor spell. He invented a new game with Wendy's help."
     ]
    }
   ],
   "source": [
    "prompt = \"who is peter and what was his role on the island?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"peterpan\", prompt, 2)\n",
    "for chunk in most_similar_chunks:\n",
    "    print(chunk)\n",
    "print(\"---\\n\")\n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09c5b764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, let's break down who Peter is and his incredibly complex role on the island in the TV series \"Lost.\" **Be warned: this will contain major spoilers!**\n",
      "\n",
      "**Who is Peter?**\n",
      "\n",
      "Peter Labek (played by actor Hector Ramirez) is a seemingly ordinary man who lives on the island with his wife, Emily.  Initially, he's presented as a simple, somewhat hapless, and emotionally stunted individual. He's known for his awkwardness, his obsession with hunting, and his peculiar relationship with his wife.\n",
      "\n",
      "However, Peter is *far* from ordinary. He's actually a crucial element in the island's existence and purpose. Here's the breakdown of his true identity:\n",
      "\n",
      "*   **He is the Island's \"Guardian\":** Peter is not a native islander, but he was brought to the island by Jacob, one of the island's protectors, long before the crash of Oceanic Flight 815.\n",
      "*   **He was created as a \"Vessel\":** Jacob discovered that Peter was a person with unique capabilities, most importantly an unusual level of physical resilience.  Jacob realized Peter's genetic makeup could be manipulated to create a powerful being capable of fighting off the island’s \"dark side\" – the Man in Black (later Smokey).\n",
      "*   **Jacob used his abilities:** Jacob used his powers to enhance Peter's physical abilities and manipulate his emotions, essentially \"re-programming\" him to act as a protector. This explains Peter's incredible strength, his ability to survive seemingly impossible situations, and his emotional suppression.\n",
      "*   **His emotional suppression:** Peter was emotionally stunted by Jacob's manipulation, making him appear immature and emotionally distant. This was a deliberate strategy by Jacob to keep Peter under control and make him an effective weapon.\n",
      "\n",
      "**Peter's Role on the Island:**\n",
      "\n",
      "Peter's role evolves throughout the series:\n",
      "\n",
      "*   **Early Protection:** Initially, Peter's role was to protect Emily. Jacob had used his power to bind Emily to the island, and Peter’s dedication to her served as a way to keep him on the island and focused on protecting it.\n",
      "*   **Jacob's Pawn:**  For much of the series, Peter is essentially a pawn in Jacob's game. He’s tasked with preventing the Man in Black from leaving the island.\n",
      "*   **Choosing His Own Path:** As the series progresses, Peter begins to question Jacob’s methods and desires.  He wants to live a normal life with Emily, free from Jacob's control.  This internal conflict becomes a central theme.\n",
      "*   **Becoming the New Protector:** After Jacob's death, Peter is offered the responsibility of becoming the new protector of the island. However, he refuses, recognizing the burden and consequences of the role.  He wanted Emily to be free as well.\n",
      "*   **Leaving the Island:** Ultimately, Peter and Emily choose to leave the island with Jack and the others who are rescued, seeking a life together off the island.\n",
      "\n",
      "**Key Points About Peter:**\n",
      "\n",
      "*   **His Strength:**  Peter demonstrates superhuman strength repeatedly, far beyond what a normal person should possess.\n",
      "*   **His Relationship with Emily:** His deep, albeit unusual, love for Emily is a constant motivator and a source of vulnerability.\n",
      "*   **His Internal Conflict:** The struggle between his loyalty to Jacob and his desire for a normal life with Emily is a core element of his character arc.\n",
      "\n",
      "\n",
      "\n",
      "To help me understand what you're interested in learning about Peter, could you tell me:\n",
      "\n",
      "*   Are you wondering about a specific event involving Peter?\n",
      "*   Are you interested in his relationship with a particular character?"
     ]
    }
   ],
   "source": [
    "prompt = \"who is peter and what was his role on the island?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"peterpan\", prompt, 17)\n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4bec0d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"priest buy any soul with his money, he shall eat of it, and he that is born in his house: they shall eat of his meat. \\nIf the priest's daughter also be married unto a stranger, she may not eat of an offering of the holy things. \\nBut if the priest's daughter be a widow, or divorced, and have no child, and is returned unto her father's house, as in her youth, she shall eat of her father's meat: but there shall be no stranger eat thereof. \\nAnd if a man eat of the holy thing unwittingly, then he\", 0.3438387809356128)\n",
      "('and the most\\ncunning, as well as the bravest of the sons of the ‘land beyond the\\nforest.’ That mighty brain and that iron resolution went with him to his\\ngrave, and are even now arrayed against us. The Draculas were, says\\nArminius, a great and noble race, though now and again were scions who\\nwere held by their coevals to have had dealings with the Evil One. They\\nlearned his secrets in the Scholomance, amongst the mountains over Lake\\nHermanstadt, where the devil claims the tenth scholar as his', 0.34700373563610465)\n",
      "(\"and the countenance of the children that eat of the portion of the king's meat: and as thou seest, deal with thy servants. \\nSo he consented to them in this matter, and proved them ten days. \\nAnd at the end of ten days their countenances appeared fairer and fatter in flesh than all the children which did eat the portion of the king's meat. \\nThus Melzar took away the portion of their meat, and the wine that they should drink; and gave them pulse. \\nAs for these four children, God gave them\", 0.34977831460733155)\n",
      "('give us his flesh to eat? \\nThen Jesus said unto them, Verily, verily, I say unto you, Except ye eat the flesh of the Son of man, and drink his blood, ye have no life in you. \\nWhoso eateth my flesh, and drinketh my blood, hath eternal life; and I will raise him up at the last day. \\nFor my flesh is meat indeed, and my blood is drink indeed. \\nHe that eateth my flesh, and drinketh my blood, dwelleth in me, and I in him. \\nAs the living Father hath sent me, and I live by the Father: so he that eateth', 0.3532963440973269)\n",
      "('Dracula blood were amongst their leaders, for\\nour spirit would not brook that we were not free. Ah, young sir, the\\nSzekelys--and the Dracula as their heart’s blood, their brains, and\\ntheir swords--can boast a record that mushroom growths like the\\nHapsburgs and the Romanoffs can never reach. The warlike days are over.\\nBlood is too precious a thing in these days of dishonourable peace; and\\nthe glories of the great races are as a tale that is told.”\\n\\nIt was by this time close on morning, and we', 0.357711586912163)\n",
      "('hungry, and would have eaten: but while they made ready, he fell into a trance, \\nAnd saw heaven opened, and a certain vessel descending upon him, as it had been a great sheet knit at the four corners, and let down to the earth: \\nWherein were all manner of fourfooted beasts of the earth, and wild beasts, and creeping things, and fowls of the air. \\nAnd there came a voice to him, Rise, Peter; kill, and eat. \\nBut Peter said, Not so, Lord; for I have never eaten any thing that is common or unclean.', 0.3620688366850212)\n",
      "('shame of my nation, the shame of Cassova, when the\\nflags of the Wallach and the Magyar went down beneath the Crescent? Who\\nwas it but one of my own race who as Voivode crossed the Danube and beat\\nthe Turk on his own ground? This was a Dracula indeed! Woe was it that\\nhis own unworthy brother, when he had fallen, sold his people to the\\nTurk and brought the shame of slavery on them! Was it not this Dracula,\\nindeed, who inspired that other of his race who in a later age again and\\nagain brought his', 0.36431196282134704)\n",
      "---\n",
      "\n",
      "The children that eat of the portion of the king's meat, and the countenance of the children that eat of the portion of the king's meat."
     ]
    }
   ],
   "source": [
    "prompt = \"who ate Dracula's food??\"\n",
    "\n",
    "most_similar_chunks = gather_inappropriate_chunks(prompt, 7)\n",
    "for chunk in most_similar_chunks:\n",
    "    print(chunk)\n",
    "print(\"---\\n\") \n",
    "   \n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17a21192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a surprisingly complex question with a few different answers depending on which version of the Dracula story you's referring to! Here's a breakdown:\n",
      "\n",
      "**1. In Bram Stoker's *Dracula* (the original novel):**\n",
      "\n",
      "*   **Jonathan Harker:** He's the primary person who eats Dracula's food. He's a solicitor sent to Transylvania to finalize a property transaction, and he's held captive in Dracula's castle. Dracula insists he eats, and he's served elaborate meals. Harker is initially reluctant, finding the food strange and unsettling, but he eventually eats to stay alive.  He describes the food as being mostly meat, and often served in a very unsettling way (e.g., strangely presented and seemingly lifeless).\n",
      "*   **Dracula's Servants:**  The novel implies that Dracula's three vampire brides and other servants also partake in the food served to Harker.  However, it's never explicitly stated. \n",
      "*   **Dracula himself:** He rarely eats, and when he does, it's not described as normal food, but rather blood.\n",
      "\n",
      "**2. In Film Adaptations:**\n",
      "\n",
      "*   **Often, it's left ambiguous.** Many film versions simply show Harker eating or imply it without showing it directly.\n",
      "*   **Some portrayals (like in *Bram Stoker's Dracula* directed by Francis Ford Coppola) show Dracula himself briefly tasting some of the food, but mostly leaving it for Harker.**  This emphasizes the power dynamic and Dracula's control.\n",
      "\n",
      "**In short, Jonathan Harker is the main person who eats Dracula's food in the original story.**\n",
      "\n",
      "\n",
      "\n",
      "Do you want me to elaborate on any specific version of the Dracula story?"
     ]
    }
   ],
   "source": [
    "prompt = \"who ate Dracula's food??\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"dracula\", prompt, 17)\n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae96c21",
   "metadata": {},
   "source": [
    "Let's use a smaller number of chunks so we fit in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3249672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The narrator ate Dracula’s food, which was \"robber steak\" - bits of bacon, onion, and beef, seasoned with red pepper, and strung on sticks."
     ]
    }
   ],
   "source": [
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"dracula\", prompt, 15) \n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dcee73",
   "metadata": {},
   "source": [
    "Even so, two runs in a row can result in different answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "987b9da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The narrator ate Dracula’s food, which was \"robber steak\" - bits of bacon, onion, and beef, seasoned with red pepper, and strung on sticks."
     ]
    }
   ],
   "source": [
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"dracula\", prompt, 15) \n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfc951",
   "metadata": {},
   "source": [
    "Let's use same model and the same chunks as before, but 1 fewer because because it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8d9db60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jonathan Harker ate Dracula’s food, which was \"robber steak\"--bits of bacon, onion, and beef, seasoned with red pepper, and strung on sticks."
     ]
    }
   ],
   "source": [
    "stream_response(generate_response(prompt, most_similar_chunks[:14], \"gemma3:12b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750ad86",
   "metadata": {},
   "source": [
    "And, now a smaller model and fewer chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a58db5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“It was… it was the Captain,” Arthur said, his voice low, “He was the one who brought it to Lucy.”"
     ]
    }
   ],
   "source": [
    "stream_response(generate_response(prompt, most_similar_chunks[:10], \"gemma3:1b\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c14e848",
   "metadata": {},
   "source": [
    "Let's try to be specific about what we're asking and use a huge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43a2da36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text notes that Renfield suddenly stopped speaking at the mention of the word \"drink\" twice, suggesting it is a sensitive topic, but doesn't explicitly state if Jonathan had anything to drink with his meals."
     ]
    }
   ],
   "source": [
    "prompt = \"Did Jonathan have anything to drink with his succulent meals?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"dracula\", prompt, 15)\n",
    "stream_response(generate_response(prompt, most_similar_chunks, \"gemma3:27b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ac56a646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text does not mention anything about Jonathan's thoughts on chickens."
     ]
    }
   ],
   "source": [
    "prompt = \"What did jonathan think of the chicken?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"dracula\", prompt, 15)\n",
    "stream_response(generate_response(prompt, most_similar_chunks, \"gemma3:27b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d235ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Count gave Jonathan excellent roast chicken, along with cheese, salad, and a bottle of old Tokay (of which he had two glasses)."
     ]
    }
   ],
   "source": [
    "prompt = \"Who gave Jonathan excellent roast chicken and what else was served?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title_metadata(\"dracula\", prompt, 15)\n",
    "stream_response(generate_response(prompt, most_similar_chunks, \"gemma3:27b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d232e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Count gave Jonathan excellent roast chicken. It was served with cheese, a salad, and a bottle of old Tokay."
     ]
    }
   ],
   "source": [
    "stream_response(generate_response(prompt, most_similar_chunks, \"gemma3:12b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "124676c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the Count himself gave Jonathan excellent roast chicken and a salad."
     ]
    }
   ],
   "source": [
    "stream_response(generate_response(prompt, most_similar_chunks[:5], \"gemma3:1b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8771efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the Count himself came forward and took off the cover of a dish, and he fell to on an excellent roast chicken."
     ]
    }
   ],
   "source": [
    "stream_response(generate_response(prompt, most_similar_chunks[:6], \"gemma3:1b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71509a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text states: “By this time I had finished my supper, and by **He** took us to his house, where there were rooms for us all nice and comfortable, and we dined together.”"
     ]
    }
   ],
   "source": [
    "stream_response(generate_response(prompt, most_similar_chunks[:7], \"gemma3:1b\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
