{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafd66d5",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Rename compose.yaml.default to compose.yaml and edit it if needed. As it stands, it will launch the vectorizer, the database, and ollama.\n",
    "\n",
    "Execute this to start the containers:\n",
    "```sh\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### By the way, if you wreck your install and need to start over:\n",
    "```sh\n",
    "docker compose down\n",
    "docker volume rm pgai_data\n",
    "docker rm pgai_db_1 pgai_vectorizer-worker_1 pgai_ollama_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e64a24",
   "metadata": {},
   "source": [
    "### Setup your PSQL instance\n",
    "\n",
    "```sh\n",
    "docker compose exec db psql\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Install pgai\n",
    "```sql\n",
    "CREATE EXTENSION IF NOT EXISTS ai CASCADE;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "While you're there, create the database table\n",
    "```sql\n",
    "CREATE TABLE books (\n",
    "    id          TEXT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n",
    "    filename    TEXT,\n",
    "    author      TEXT,\n",
    "    title       TEXT NOT NULL,\n",
    "    text        TEXT NOT NULL\n",
    ");\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "And, before you quit, create the vectorizer\n",
    "```sql\n",
    "SELECT ai.create_vectorizer(\n",
    "     'texts'::regclass,\n",
    "     destination => 'texts_embeddings',\n",
    "     embedding => ai.embedding_ollama('nomic-embed-text', 768),\n",
    "     chunking => ai.chunking_recursive_character_text_splitter('text')\n",
    ");\n",
    "```\n",
    "\n",
    "*Keep in mind that for other texts, you can customize the chunking and embedding models.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faddfe8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "If you've got a spare terminal window, you can tail the vectorizer logs\n",
    "```sh\n",
    "docker compose logs -f vectorizer-worker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd35645d",
   "metadata": {},
   "source": [
    "---\n",
    "Let's get our imports out of the way. Run this.\n",
    "\n",
    "In your virtual environment, run the following to install the necessary libraries.\n",
    "```sh\n",
    "python -m pip install numpy ollama langchain_text_splitters python-dotenv pandas psycopg2-binary Jinja2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274aac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os.path\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "from numpy.linalg import norm\n",
    "from ollama import Client\n",
    "import time\n",
    "import numpy as np\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc409da",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect_db\u001b[39m():\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m psycopg2.connect(DB_String)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m connect_db() \u001b[38;5;28;01mas\u001b[39;00m connection:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m connection.cursor() \u001b[38;5;28;01mas\u001b[39;00m cursor:\n\u001b[32m     14\u001b[39m         cursor.execute(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m                       CREATE EXTENSION IF NOT EXISTS ai CASCADE;\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m                       \u001b[39m\u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1470\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1575\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1950\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeff\\IdeaProjects\\JeffsLLMExperiments\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jeff\\IdeaProjects\\JeffsLLMExperiments\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "OLLAMA_HOST=os.environ.get(\"OLLAMA_HOST\")\n",
    "DB_String=os.environ.get(\"DATABASE_CONNECTION_STRING\")\n",
    "\n",
    "def connect_db():\n",
    "    return psycopg2.connect(DB_String)\n",
    "\n",
    "with connect_db() as connection:\n",
    "    with connection.cursor() as cursor:\n",
    "        cursor.execute(\"\"\"\n",
    "                       CREATE EXTENSION IF NOT EXISTS ai CASCADE;\n",
    "                       \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "                       CREATE TABLE texts (\n",
    "                        id          BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n",
    "                        filename    TEXT,\n",
    "                        author      TEXT,\n",
    "                        title       TEXT NOT NULL,\n",
    "                        text        TEXT NOT NULL\n",
    "                        );\n",
    "                       \"\"\")\n",
    "\n",
    "        cursor.execute(\"\"\"\n",
    "                       SELECT ai.create_vectorizer(\n",
    "                           'texts'::regclass,\n",
    "                            destination => 'texts_embeddings',\n",
    "                            embedding => ai.embedding_ollama('nomic-embed-text', 768),\n",
    "                            chunking => ai.chunking_recursive_character_text_splitter('text', 500, 10, separators => array[E'\\n;', ' '])\n",
    "                        );\n",
    "                       \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a20fb4",
   "metadata": {},
   "source": [
    "Now, load some data into the **texts** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d66d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barrie-peterpan\n",
      "god-bible\n",
      "god-world192\n",
      "stoker-dracula\n",
      "shakespeare-a lover's complaint\n",
      "shakespeare-all's well that ends well\n",
      "shakespeare-antony and cleopatra\n",
      "shakespeare-as you like it\n",
      "shakespeare-comedy of errors\n",
      "shakespeare-coriolanus\n",
      "shakespeare-cymbeline\n",
      "shakespeare-hamlet\n",
      "shakespeare-julius caesar\n",
      "shakespeare-king henry iv, part 1\n",
      "shakespeare-king henry iv, part 2\n",
      "shakespeare-king henry v\n",
      "shakespeare-king henry vi, part 1\n",
      "shakespeare-king henry vi, part 2\n",
      "shakespeare-king henry vi, part 3\n",
      "shakespeare-king henry viii\n",
      "shakespeare-king john\n",
      "shakespeare-king lear\n",
      "shakespeare-king richard ii\n",
      "shakespeare-king richard iii\n",
      "shakespeare-love's labour's lost\n",
      "shakespeare-lucrece\n",
      "shakespeare-macbeth\n",
      "shakespeare-measure for measure\n",
      "shakespeare-merchant of venice\n",
      "shakespeare-merry wives of windsor\n",
      "shakespeare-midsummer night's dream\n",
      "shakespeare-much ado about nothing\n",
      "shakespeare-othello\n",
      "shakespeare-pericles, prince of tyre\n",
      "shakespeare-romeo and juliet\n",
      "shakespeare-sonnets\n",
      "shakespeare-taming of the shrew\n",
      "shakespeare-tempest\n",
      "shakespeare-timon of athens\n",
      "shakespeare-titus andronicus\n",
      "shakespeare-troilus and cressida\n",
      "shakespeare-twelfth night\n",
      "shakespeare-two gentlemen of verona\n",
      "shakespeare-various\n",
      "shakespeare-venus and adonis\n",
      "shakespeare-winter's tale\n",
      "startrek-fc\n",
      "startrek-gens\n",
      "startrek-ins\n",
      "startrek-nem\n",
      "startrek-tff\n",
      "startrek-tmp\n",
      "startrek-tsfs\n",
      "startrek-tuc\n",
      "startrek-tvh\n",
      "startrek-twok\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "class Text:\n",
    "    def __init__(self, filename):\n",
    "        self.title = re.match(r\".+\\\\(.+)\\.txt$\", filename).group(1)\n",
    "        self.author = re.match(r\".+\\\\(\\w+)\\\\.+.txt\", filename).group(1)\n",
    "        self.filename = f\"{self.author}-{self.title}\"\n",
    "\n",
    "        with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "            file_contents = f.read()\n",
    "            self.contents = re.sub(r'[^\\S\\r\\n]+', \" \", file_contents)\n",
    "        # print(f\"{self.title} by {self.author}\")\n",
    "        # print(self.contents[:30])\n",
    "\n",
    "def load_directory(dirname, filter):\n",
    "    return list(Path(dirname).rglob(filter))\n",
    "\n",
    "# This method has an intentional lazy flaw in that if a record is already in the database, we just leave it alone for now.\n",
    "def add_text_record(_text):\n",
    "    print(_text.filename)\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT filename from texts where filename = %s\", [_text.filename])\n",
    "            if(len(cursor.fetchall()) == 0):\n",
    "                cursor.execute(\"INSERT into texts (filename, author, title, text)\" \\\n",
    "                \"VALUES (%s, %s, %s, %s)\", (_text.filename, _text.author, _text.title, _text.contents))\n",
    "\n",
    "files = load_directory(\"texts\", \"*.txt\")\n",
    "# Modify this to limit how many texts to vectorize\n",
    "for file in files:\n",
    "    text = Text(str(file))\n",
    "    # We aren't creating an array of Text objects because it could consume an\n",
    "    # outrageous amount of memory if there are tons of texts.\n",
    "    add_text_record(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8531802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n",
      "496\n",
      "496\n",
      "497\n",
      "495\n",
      "---\n",
      "499\n",
      "496\n",
      "496\n",
      "497\n",
      "495\n",
      "---\n",
      "499\n",
      "496\n",
      "496\n",
      "499\n",
      "494\n"
     ]
    }
   ],
   "source": [
    "def gather_appropriate_chunks_by_author(author, prompt, count):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                           SELECT \n",
    "                           chunk,\n",
    "                           embedding <=> ai.ollama_embed('nomic-embed-text', %s) as distance\n",
    "                           FROM texts_embeddings\n",
    "                           WHERE author = %s\n",
    "                           ORDER BY distance\n",
    "                           LIMIT %s;\n",
    "                           \"\"\", (prompt, author, count))\n",
    "            return cursor.fetchall()    \n",
    "\n",
    "def gather_appropriate_chunks_by_title(title, prompt, count):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                           SELECT \n",
    "                           chunk,\n",
    "                           embedding <=> ai.ollama_embed('nomic-embed-text', %s) as distance\n",
    "                           FROM texts_embeddings\n",
    "                           WHERE title = %s\n",
    "                           ORDER BY distance\n",
    "                           LIMIT %s;\n",
    "                           \"\"\", (prompt, title, count))\n",
    "            return cursor.fetchall() \n",
    "\n",
    "def gather_inappropriate_chunks(prompt, count):\n",
    "    with connect_db() as connection:\n",
    "        with connection.cursor() as cursor:\n",
    "            cursor.execute(\"\"\"\n",
    "                           SELECT \n",
    "                           chunk,\n",
    "                           embedding <=> ai.ollama_embed('nomic-embed-text', %s) as distance\n",
    "                           FROM texts_embeddings\n",
    "                           ORDER BY distance\n",
    "                           LIMIT %s;\n",
    "                           \"\"\", (prompt, count))\n",
    "            return cursor.fetchall() \n",
    "            \n",
    "chunks = gather_appropriate_chunks_by_author('god', 'who is the king?', 5)\n",
    "for chunk in chunks:\n",
    "    print(len(chunk[0]))\n",
    "print(\"---\")\n",
    "\n",
    "chunks = gather_appropriate_chunks_by_title('bible', 'who is the king?', 5)\n",
    "for chunk in chunks:\n",
    "    print(len(chunk[0]))\n",
    "print(\"---\")\n",
    "\n",
    "chunks = gather_inappropriate_chunks('who is the king?', 5)\n",
    "for chunk in chunks:\n",
    "    print(len(chunk[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909f4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(\n",
    "    host='http://192.168.137.117:11434',\n",
    ")\n",
    "prompt_size=8000\n",
    "chunking_size=500\n",
    "chunk_count = prompt_size // chunking_size\n",
    "embed_model = \"nomic-embed-text\"\n",
    "generate_model = \"gemma3:12b\"\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful reading assistant who answers questions\n",
    "based on snippets of text provided in context. Answer only using the context provided,\n",
    "being as concise as possible. If the answer isn't in the context, simply say so.\n",
    "Context:\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(prompt, most_similar_chunks, model):\n",
    "    # client.pull(generate_model) # You really should pre-pull your models\n",
    "    \n",
    "    # most_similar_chunks = gather_inappropriate_chunks(prompt)\n",
    "    # for item in most_similar_chunks:\n",
    "    #     print(item[0])\n",
    "    # print(\"\\n\\n\\n\")\n",
    "    system_prompt = SYSTEM_PROMPT + \"\\n\".join(item[0] for item in most_similar_chunks)\n",
    "    # print(f\"{system_prompt}\\n\\n\")\n",
    "    \n",
    "    response = client.chat(\n",
    "        model,\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        stream = True\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def stream_response(stream):\n",
    "    for chunk in stream:\n",
    "        print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00613f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter is the \"Great White Father,\" and he is the one who forbade the boys from looking like him. He leads the island, sets rules, and \"thins\" out the lost boys when they threaten to grow up."
     ]
    }
   ],
   "source": [
    "prompt = \"who is peter and what was his role on the island?\"\n",
    "\n",
    "most_similar_chunks = gather_appropriate_chunks_by_title(\"peterpan\", prompt, 7)\n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b93a242",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"who is peter and what was his role on the island?\"\n",
    "\n",
    "most_similar_chunks = gather_inappropriate_chunks(prompt, 6)\n",
    "for chunk in most_similar_chunks:\n",
    "    print(chunk)\n",
    "    \n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec0d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"who ate Dracula's food??\"\n",
    "\n",
    "most_similar_chunks = gather_inappropriate_chunks(prompt, 7)\n",
    "for chunk in most_similar_chunks:\n",
    "    print(chunk)\n",
    "    \n",
    "stream_response(generate_response(prompt, most_similar_chunks, generate_model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
